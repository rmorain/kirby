# Kirby
> Training a transformer language model with an augmented dataset. The augmented dataset is generated by extracting knowledge from the wikidata knowledge base.


## Database

I mount the database from mind.cs.byu to my local machine.

[SSH File Drive](https://www.macissues.com/2014/10/13/how-to-mount-a-remote-system-as-a-drive-using-ssh-in-os-x/)

For Mac:

`sshfs robert@mind.cs.byu.edu:/home/data /Users/robertmorain/mind.cs.byu.edu -ovolname=MIND`

For Linux:

`sshfs robert@mind.cs.byu.edu:/home/data /home/rob/mind.cs.byu.edu -o allow_other`

In order for your docker container to see these files, you need to modify the `docker-compose.yml` at `services > fastai > volumes` `/Users/robertmorain/mind.cs.byu.edu` to match your local mounted folder.


## Docker

Start the docker container by running

`docker-compose up`

in the kirby project folder.

Go to http://127.0.0.1:8080/lab to open a jupyter lab instance to work in.

## Download data

In order for the data download script to work you need to install `unzip`

Open a terminal in Jupyter lab and run:

`apt update && apt install unzip`

Run `./bin/download_data` to download data in project directory

## Run Tests

To make sure everything is working, run tests in your Jupyter lab terminal

`nbdev_test_nbs`

It's a good idea to run this before submitting any pull requests to the `main` branch

## nbdev

If you are unfamiliar with nbdev, please refer to the [tutorial](https://nbdev.fast.ai/tutorial.html)

## Git

Before you make changes, create a branch.

I make commits outside of the Jupyter lab terminal to avoid errors with configuring my email address.

# Recreating the database

Download [Wikidata JSON dump] https://dumps.wikimedia.org/wikidatawiki/entities/



