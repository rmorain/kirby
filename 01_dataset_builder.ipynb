{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_builder\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import random\n",
    "from rake_nltk import Rake\n",
    "from kirby.database_proxy import WikiDatabase\n",
    "import json\n",
    "import importlib\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Builder\n",
    "> Builds an optimal dataset with knowledge base relations, from a vanilla dataset.\n",
    "\n"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DatasetBuilder():\n",
    "    \"Build a dataset using `get_entities_in_text`\"\n",
    "    def __init__(self):\n",
    "        self.rake = Rake()\n",
    "        self.db = WikiDatabase()\n",
    "        self.nlp = en_core_web_sm.load()\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "        self.encoder = hub.load(module_url)\n",
    "    \n",
    "    def build(self, ds, dataset_type='random'):\n",
    "        \"Build a database based a given dataset\"\n",
    "        if dataset_type == 'random':\n",
    "            ds.map(self.random, batched=False)\n",
    "        elif dataset_type == 'description':\n",
    "            pass\n",
    "        elif dataset_type == 'relevant':\n",
    "            pass\n",
    "        \n",
    "    def keyword(self, x):\n",
    "        ranked_phrases = self.get_ranked_phrases(x)\n",
    "        return ranked_phrases[0]\n",
    "    \n",
    "    def get_ranked_phrases(self, x):\n",
    "        self.rake.extract_keywords_from_text(x)\n",
    "        return self.rake.get_ranked_phrases()\n",
    "    \n",
    "    #staticmethod\n",
    "    def add_to_accepted(self, a_sentences, sentence):\n",
    "        if len(a_sentences) > 2:\n",
    "            a_sentences.pop(0)\n",
    "        a_sentences.append(sentence)\n",
    "    \n",
    "\n",
    "    def get_entities_in_text(self, text):\n",
    "        \"Returns entities found in the sentence `x`\"\n",
    "        doc = self.nlp(x)\n",
    "        entities = []\n",
    "        spacy_entities = doc.ents\n",
    "        for entity in spacy_entities: \n",
    "            entity = self.db.get_entity_by_label(entity.text)\n",
    "            entities.append(entity)\n",
    "        return entities\n",
    "\n",
    "    def entity(self, ranked_phrases):\n",
    "        \"Queries the knowledge base to find the entity and it's relations\"\n",
    "        for phrase in ranked_phrases:\n",
    "            entity = self.kba.get_entity(phrase)\n",
    "            if entity is not None:\n",
    "                return entity\n",
    "        return entity   \n",
    "    def get_entity_associations(self, entity_id):\n",
    "        \"\"\"\n",
    "        Given an `entity_id` return a dictionary containing all the associated properties.\n",
    "        \"\"\"\n",
    "        entity_associations_dict = {}\n",
    "        # Remove all None values from list\n",
    "        associations = self.db.get_entity_associations(entity_id)\n",
    "        for property_id, related_entity_id in associations: \n",
    "            property_name, related_entity_label = self.db.get_property_string(property_id, related_entity_id)\n",
    "            entity_associations_dict[property_name] = related_entity_label\n",
    "        return entity_associations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlite3.Connection object at 0x7f6bce64fc60>\n"
     ]
    }
   ],
   "source": [
    "# creation\n",
    "ds_builder = DatasetBuilder()\n",
    "assert isinstance(ds_builder, DatasetBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ranked phrases\n",
    "x = \"Stephen Curry is my favorite basketball player.\"\n",
    "ds_builder.rake.extract_keywords_from_text(x)\n",
    "ranked_phrases = ds_builder.rake.get_ranked_phrases()\n",
    "assert ranked_phrases == ['favorite basketball player', 'stephen curry'], \"RAKE failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q11571', 'Cristiano Ronaldo', 'Portuguese association football player']\n"
     ]
    }
   ],
   "source": [
    "print(ds_builder.db.get_entity_by_label('Cristiano Ronaldo'))\n",
    "assert ds_builder.db.get_entity_by_label('Cristiano Ronaldo') == ['Q11571', 'Cristiano Ronaldo', 'Portuguese association football player'], 'ERROR in `database_proxy`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Q2283', 'Microsoft', 'American multinational technology corporation'], ['Q224892', 'Bethesda', 'Wikimedia disambiguation page']]\n"
     ]
    }
   ],
   "source": [
    "# Get Entities from the sentence\n",
    "x = \"Microsoft has bought Bethesda\"\n",
    "entities = ds_builder.get_entities_in_text(x)\n",
    "print(entities)\n",
    "assert entities == [['Q2283', 'Microsoft', 'American multinational technology corporation'],\\\n",
    "                    ['Q224892', 'Bethesda', 'Wikimedia disambiguation page']],\\\n",
    "                    'Error in `dataset_builder.get_entities_in_text()`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get associations from an entity\n",
    "associations = ds_builder.get_entity_associations(entities[0][0])\n",
    "assert associations == {\"topic's main Wikimedia portal\": 'Portal:Microsoft',\n",
    " 'founded by': 'Bill Gates',\n",
    " 'country': 'United States of America',\n",
    " 'instance of': 'software company',\n",
    " 'headquarters location': 'Redmond',\n",
    " 'stock exchange': 'NASDAQ',\n",
    " 'chief executive officer': 'Steve Ballmer',\n",
    " \"topic's main category\": 'Category:Microsoft',\n",
    " 'subsidiary': 'Xbox Game Studios',\n",
    " 'described by source': 'Lentapedia (full versions)',\n",
    " 'industry': 'technology industry',\n",
    " 'product or material produced': 'Microsoft Windows',\n",
    " 'legal form': 'Washington corporation',\n",
    " 'business division': 'Microsoft Research',\n",
    " 'history of topic': 'history of Microsoft',\n",
    " 'member of': 'Alliance for Open Media',\n",
    " 'permanent duplicated item': None,\n",
    " 'part of': 'NASDAQ-100',\n",
    " 'award received': 'Big Brother Awards',\n",
    " 'owner of': 'Microsoft TechNet',\n",
    " 'owned by': 'BlackRock',\n",
    " 'board member': 'Reid Hoffman',\n",
    " 'chairperson': 'John W. Thompson',\n",
    " 'location of formation': 'Albuquerque',\n",
    " 'director / manager': 'Satya Nadella',\n",
    " 'external auditor': 'Deloitte & Touche LLP',\n",
    " 'partnership with': 'ID2020'}, 'Error in `dataset_builder.get_entity_associations()`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
