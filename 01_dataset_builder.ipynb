{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_builder\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import random\n",
    "from rake_nltk import Rake\n",
    "from kirby.database_proxy import WikiDatabase\n",
    "import json\n",
    "import importlib\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Builder\n",
    "> Builds a number of types of datasets that are augmented by the `wikidata` knowledge base\n",
    "\n",
    "## Dataset Variations\n",
    "\n",
    "Keywords can be supplied to the `build` function through the `dataset_type` keyword argument\n",
    "\n",
    "### Description\n",
    "\n",
    "`DatasetBuilder.build(ds, dataset_type='description')`\n",
    "\n",
    "Augements the given dataset with the description of the `keyword`. \n",
    "\n",
    "*Example*\n",
    "\n",
    "`Stephen Curry is my favorite basketball player. {Stephen Curry: {Description: American basketball player}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DatasetBuilder():\n",
    "    \"Build a dataset using `get_entities_in_text`\"\n",
    "    def __init__(self):\n",
    "        self.rake = Rake()\n",
    "        self.db = WikiDatabase()\n",
    "        self.nlp = en_core_web_sm.load()\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "        self.encoder = hub.load(module_url)\n",
    "    \n",
    "    def build(self, ds, dataset_type='random'):\n",
    "        \"Build a database based a given dataset\"\n",
    "        if dataset_type == 'random':\n",
    "            return ds.map(self.random, batched=False)\n",
    "        elif dataset_type == 'description':\n",
    "            return ds.map(self.description, batched=False)\n",
    "        elif dataset_type == 'relevant':\n",
    "            pass\n",
    "        \n",
    "    def description(self, sequence):\n",
    "        \"\"\"Return a description augmented version of the seqeuence `text`\"\"\"\n",
    "        text = sequence['text']\n",
    "        try:\n",
    "            keyword_entity = self._keyword_entity(text)\n",
    "            json_string = self._get_json(keyword_entity)\n",
    "            # Return concatenated string\n",
    "            sequence['text'] += \" \" + json_string\n",
    "        except Exception as e:\n",
    "            # We expect there to be an exception when no entities are found\n",
    "            pass\n",
    "        return sequence\n",
    "    \n",
    "    def _keyword_entity(self, text):\n",
    "        \"\"\"Return the entity of the highest ranked keyword\"\"\"\n",
    "        ranked_phrases = self.get_ranked_phrases(text)\n",
    "        for phrase in ranked_phrases:\n",
    "            entity = self.db.get_entity_by_label(phrase)\n",
    "            if entity:\n",
    "                return entity\n",
    "        return None\n",
    "        \n",
    "    def _get_json(self, item):\n",
    "        \"\"\"Return JSON version of list object\"\"\"\n",
    "        d = {\"label\": None, \"description\": None}\n",
    "        d['label'] = item[1]\n",
    "        d['description'] = item[2]\n",
    "        return json.dumps(d)\n",
    "        \n",
    "    def keyword(self, x):\n",
    "        ranked_phrases = self.get_ranked_phrases(x)\n",
    "        return ranked_phrases[0]\n",
    "    \n",
    "    def get_ranked_phrases(self, x):\n",
    "        self.rake.extract_keywords_from_text(x)\n",
    "        return self.rake.get_ranked_phrases()\n",
    "    \n",
    "    #staticmethod\n",
    "    def add_to_accepted(self, a_sentences, sentence):\n",
    "        if len(a_sentences) > 2:\n",
    "            a_sentences.pop(0)\n",
    "        a_sentences.append(sentence)\n",
    "    \n",
    "\n",
    "    def get_entities_in_text(self, text):\n",
    "        \"Returns entities found in the sentence `text`\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "        spacy_entities = doc.ents\n",
    "        for entity in spacy_entities: \n",
    "            entity = self.db.get_entity_by_label(entity.text)\n",
    "            if entity:\n",
    "                entities.append(entity)\n",
    "        return entities  \n",
    "    \n",
    "    def get_entity_associations(self, entity_id):\n",
    "        \"\"\"\n",
    "        Given an `entity_id` return a dictionary containing all the associated properties.\n",
    "        \"\"\"\n",
    "        entity_associations_dict = {}\n",
    "        # Remove all None values from list\n",
    "        associations = self.db.get_entity_associations(entity_id)\n",
    "        for property_id, related_entity_id in associations: \n",
    "            property_name, related_entity_label = self.db.get_property_string(property_id, related_entity_id)\n",
    "            entity_associations_dict[property_name] = related_entity_label\n",
    "        return entity_associations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlite3.Connection object at 0x7fab329612d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "WARNING:datasets.builder:Using custom data configuration default-9b5839420c3d088b\n",
      "WARNING:datasets.builder:Reusing dataset text (/home/rob/.cache/huggingface/datasets/text/default-9b5839420c3d088b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<bound method DatasetBuilder.description of <__main__.DatasetBuilder object at 0x7faa9eca8b00>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7719c46126447499d30341a1cac843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18014.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-9b5839420c3d088b\n",
      "WARNING:datasets.builder:Reusing dataset text (/home/rob/.cache/huggingface/datasets/text/default-9b5839420c3d088b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26c40b28429442393a460447f7a8e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10776"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build description dataset\n",
    "from kirby.run_params import RunParams\n",
    "from kirby.data_manager import DataManager\n",
    "from datasets import load_dataset\n",
    "\n",
    "run_params = RunParams(debug=False)\n",
    "data_manager = DataManager(run_params)\n",
    "ds_builder = DatasetBuilder()\n",
    "block_size = 128\n",
    "\n",
    "split = 'train'\n",
    "split = f'{split}[:{run_params.batch_size*block_size if run_params.debug else f\"{run_params.data_set_percentage}%\"}]'\n",
    "ds = load_dataset('text', data_files=run_params.data_files, split=split)\n",
    "aug_train_ds = ds_builder.build(ds, dataset_type='description')\n",
    "\n",
    "# Save\n",
    "aug_train_ds.to_csv('data/augmented_datasets/description_train.csv')\n",
    "\n",
    "split = 'valid'\n",
    "split = f'{split}[:{run_params.batch_size*block_size if run_params.debug else f\"{run_params.data_set_percentage}%\"}]'\n",
    "ds = load_dataset('text', data_files=run_params.data_files, split=split)\n",
    "aug_valid_ds = ds_builder.build(ds, dataset_type='description')\n",
    "\n",
    "# Save\n",
    "aug_valid_ds.to_csv('data/augmented_datasets/description_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation\n",
    "ds_builder = DatasetBuilder()\n",
    "assert isinstance(ds_builder, DatasetBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ranked phrases\n",
    "x = \"Stephen Curry is my favorite basketball player.\"\n",
    "ds_builder.rake.extract_keywords_from_text(x)\n",
    "ranked_phrases = ds_builder.rake.get_ranked_phrases()\n",
    "assert ranked_phrases == ['favorite basketball player', 'stephen curry'], \"RAKE failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_builder.db.get_entity_by_label('Cristiano Ronaldo'))\n",
    "assert ds_builder.db.get_entity_by_label('Cristiano Ronaldo') == ['Q11571', 'Cristiano Ronaldo', 'Portuguese association football player'], 'ERROR in `database_proxy`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entities from the sentence\n",
    "x = \"Microsoft has bought Bethesda\"\n",
    "entities = ds_builder.get_entities_in_text(x)\n",
    "print(entities)\n",
    "assert entities == [['Q2283', 'Microsoft', 'American multinational technology corporation'],\\\n",
    "                    ['Q224892', 'Bethesda', 'Wikimedia disambiguation page']],\\\n",
    "                    'Error in `dataset_builder.get_entities_in_text()`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get associations from an entity\n",
    "associations = ds_builder.get_entity_associations(entities[0][0])\n",
    "assert associations == {\"topic's main Wikimedia portal\": 'Portal:Microsoft',\n",
    " 'founded by': 'Bill Gates',\n",
    " 'country': 'United States of America',\n",
    " 'instance of': 'software company',\n",
    " 'headquarters location': 'Redmond',\n",
    " 'stock exchange': 'NASDAQ',\n",
    " 'chief executive officer': 'Steve Ballmer',\n",
    " \"topic's main category\": 'Category:Microsoft',\n",
    " 'subsidiary': 'Xbox Game Studios',\n",
    " 'described by source': 'Lentapedia (full versions)',\n",
    " 'industry': 'technology industry',\n",
    " 'product or material produced': 'Microsoft Windows',\n",
    " 'legal form': 'Washington corporation',\n",
    " 'business division': 'Microsoft Research',\n",
    " 'history of topic': 'history of Microsoft',\n",
    " 'member of': 'Alliance for Open Media',\n",
    " 'permanent duplicated item': None,\n",
    " 'part of': 'NASDAQ-100',\n",
    " 'award received': 'Big Brother Awards',\n",
    " 'owner of': 'Microsoft TechNet',\n",
    " 'owned by': 'BlackRock',\n",
    " 'board member': 'Reid Hoffman',\n",
    " 'chairperson': 'John W. Thompson',\n",
    " 'location of formation': 'Albuquerque',\n",
    " 'director / manager': 'Satya Nadella',\n",
    " 'external auditor': 'Deloitte & Touche LLP',\n",
    " 'partnership with': 'ID2020'}, 'Error in `dataset_builder.get_entity_associations()`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description\n",
    "text = \"Darth Vader cut off Luke Skywalker's hand\"\n",
    "ds_builder.description(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
