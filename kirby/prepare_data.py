# AUTOGENERATED! DO NOT EDIT! File to edit: 07_prepare_data.ipynb (unless otherwise specified).

__all__ = ['prepare_data', 'tokenize', 'tokenize', 'prepare_ds']

# Cell
from datasets import load_dataset
from transformers import GPT2Tokenizer
from .run_params import RunParams

# Cell
# Load, Tokenize, and Augment data
def prepare_data(run_params):
    train_ds, val_ds = map(prepare_ds, ('train', 'validation'))
    return train_ds, val_ds

# Tokenize a sequence
def tokenize(x):
    tokens = tokenizer(
        x['statement'],
        max_length=run_params['statement_length'],
        padding=True)
    x['statement_ids'] = tokens['input_ids']
    x['statement_mask'] = tokens['attention_mask']
    return x

# Tokenize a sequence
def tokenize(x):
    tokens = tokenizer(
        x['text'],
        max_length=run_params.seq_length,
        padding=True)
    input_ids = tokens['input_ids']
    attention_mask = tokens['attention_mask']
    return input_ids, attention_mask

# Load the data
def prepare_ds(split):
    tokenizer = GPT2Tokenizer.from_pretrained(run_params['model_name'])
    tokenizer.pad_token = tokenizer.eos_token
    ds = load_dataset('text', data_files=run_params.data_files)
    ds = ds.map(tokenize, batched=True)
    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'statement_ids', 'statement_mask'])

    return ds