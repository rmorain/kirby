# AUTOGENERATED! DO NOT EDIT! File to edit: 06_data_manager.ipynb (unless otherwise specified).

__all__ = ['DataManager']

# Cell
from datasets import load_dataset
import numpy as np
import torch
from transformers import GPT2Tokenizer
from .run_params import RunParams

# Cell
class DataManager():
    def __init__(self, run_params):
        self.run_params = run_params
    # Load, Tokenize, and Augment data
    def prepare_data(self):
        train_ds, val_ds = map(self.prepare_ds, ('train', 'valid'))
        return train_ds, val_ds

    def prepare_ds(self, split):
        tokenizer = GPT2Tokenizer.from_pretrained(self.run_params.model)
        tokenizer.pad_token = tokenizer.eos_token
        split = f'{split}[:{self.run_params.batch_size if self.run_params.debug else f"{self.run_params.data_set_percentage}%"}]'
        ds = load_dataset('text', data_files=self.run_params.data_files, split=split)
        ds = ds.map(self.tokenize, batched=True, fn_kwargs={'tokenizer':tokenizer})
        ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])
        return ds

    # Tokenize a sequence
    def tokenize(self, x, tokenizer=None):
        tokens = tokenizer(
            x['text'],
            max_length=self.run_params.seq_length,
            truncation=True,
            padding=True)
        return tokens
