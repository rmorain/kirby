{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_builder\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import random\n",
    "from kirby.database_proxy import WikiDatabase\n",
    "import json\n",
    "import importlib\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Builder\n",
    "> Builds a number of types of datasets that are augmented by the `wikidata` knowledge base\n",
    "\n",
    "## Dataset Variations\n",
    "\n",
    "Keywords can be supplied to the `build` function through the `dataset_type` keyword argument\n",
    "\n",
    "### Description\n",
    "\n",
    "`DatasetBuilder.build(ds, dataset_type='description')`\n",
    "\n",
    "Augements the given dataset with the description of the `keyword`. \n",
    "\n",
    "*Example*\n",
    "\n",
    "`Stephen Curry is my favorite basketball player. {Stephen Curry: {Description: American basketball player}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DatasetBuilder():\n",
    "    \"Build a dataset using `get_entities_in_text`\"\n",
    "    def __init__(self):\n",
    "#         self.db = WikiDatabase()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "#         module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "#         self.encoder = hub.load(module_url)\n",
    "    \n",
    "    def build(self, ds, dataset_type='random'):\n",
    "        \"Build a database based a given dataset\"\n",
    "        if dataset_type == 'random':\n",
    "            return ds.map(self.random, batched=False)\n",
    "        elif dataset_type == 'description':\n",
    "            return ds.map(self.description, batched=False)\n",
    "        elif dataset_type == 'relevant':\n",
    "            pass\n",
    "        \n",
    "    def build_knowledge_entities(self, ds, split):\n",
    "        ds = ds.map(self.get_entities, batched=True, num_proc=4)\n",
    "        ds.save_to_disk('data/augmented_datasets/entities/' + split + '/')\n",
    "        \n",
    "    def build_csv(self, ds, split):\n",
    "        ds = ds.map(self.retrieve_knowledge, batched=False)\n",
    "        ds.save_to_disk('data/augmented_datasets/')\n",
    "        \n",
    "    def get_entities(self, batch):\n",
    "        import pdb; pdb.set_trace()\n",
    "        doc = self.nlp(batch)\n",
    "        entities = doc.ents\n",
    "        \n",
    "    def retrieve_knowledge(self, sequence):\n",
    "        text = sequence['text']\n",
    "        entities = self.get_entities_in_text(text)\n",
    "        knowledge = self.add_associations(entities)\n",
    "        sequence['knowledge'] = knowledge\n",
    "        return sequence\n",
    "            \n",
    "    def add_associations(self, entities):\n",
    "        \"Returns list of entity/association dictionaries\"\n",
    "        associations = []\n",
    "        for e in entities:\n",
    "            a = self.get_entity_associations(e)\n",
    "            k = {e[1]: a}\n",
    "            associations.append(k)\n",
    "        return associations\n",
    "        \n",
    "    def _get_json(self, item):\n",
    "        \"\"\"Return JSON version of list object\"\"\"\n",
    "        d = {\"label\": None, \"description\": None}\n",
    "        d['label'] = item[1]\n",
    "        d['description'] = item[2]\n",
    "        return json.dumps(d)\n",
    "    \n",
    "    def get_entities_in_text(self, text):\n",
    "        \"Returns entities found in the sentence `text`\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "        spacy_entities = doc.ents\n",
    "        for entity in spacy_entities: \n",
    "            entity = self.db.get_entity_by_label(entity.text)\n",
    "            if entity:\n",
    "                entities.append(entity)\n",
    "        return entities  \n",
    "    \n",
    "    def get_entity_associations(self, entity):\n",
    "        \"\"\"\n",
    "        Given an `entity_id` return a dictionary containing all the associated properties.\n",
    "        \"\"\"\n",
    "        entity_id = entity[0]\n",
    "        entity_associations_dict = {'id':entity_id, 'description':entity[2]}\n",
    "        # Remove all None values from list\n",
    "        associations = self.db.get_entity_associations(entity_id)\n",
    "        if not associations:\n",
    "            return None\n",
    "        for property_id, related_entity_id in associations: \n",
    "            property_name, related_entity_label = self.db.get_property_string(property_id, related_entity_id)\n",
    "            entity_associations_dict[property_name] = related_entity_label\n",
    "        return entity_associations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d64f335cc8a13d66\n",
      "Reusing dataset text (/home/rob/.cache/huggingface/datasets/text/default-d64f335cc8a13d66/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Loading cached processed dataset at /home/rob/.cache/huggingface/datasets/text/default-d64f335cc8a13d66/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-afe4d9c5e6fd685f.arrow\n"
     ]
    }
   ],
   "source": [
    "# Build description dataset\n",
    "from kirby.run_params import RunParams\n",
    "from kirby.data_manager import DataManager\n",
    "from datasets import load_dataset\n",
    "\n",
    "run_params = RunParams(debug=True)\n",
    "data_manager = DataManager(run_params)\n",
    "ds_builder = DatasetBuilder()\n",
    "\n",
    "split = 'train'\n",
    "ds = data_manager.load(split)\n",
    "ds_builder.build_knowledge_entities(ds, split)\n",
    "\n",
    "split = 'valid'\n",
    "ds = data_manager.load(split)\n",
    "ds_builder.build_knowledge_entities(ds, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 490\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation\n",
    "ds_builder = DatasetBuilder()\n",
    "\n",
    "assert isinstance(ds_builder, DatasetBuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ranked phrases\n",
    "x = \"Stephen Curry is my favorite basketball player.\"\n",
    "ds_builder.rake.extract_keywords_from_text(x)\n",
    "ranked_phrases = ds_builder.rake.get_ranked_phrases()\n",
    "assert ranked_phrases == ['favorite basketball player', 'stephen curry'], \"RAKE failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_builder.db.get_entity_by_label('Cristiano Ronaldo'))\n",
    "assert ds_builder.db.get_entity_by_label('Cristiano Ronaldo') == ['Q11571', 'Cristiano Ronaldo', 'Portuguese association football player'], 'ERROR in `database_proxy`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entities from the sentence\n",
    "x = \"Microsoft has bought Bethesda\"\n",
    "entities = ds_builder.get_entities_in_text(x)\n",
    "print(entities)\n",
    "assert entities == [['Q2283', 'Microsoft', 'American multinational technology corporation'],\\\n",
    "                    ['Q224892', 'Bethesda', 'Wikimedia disambiguation page']],\\\n",
    "                    'Error in `dataset_builder.get_entities_in_text()`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get associations from an entity\n",
    "associations = ds_builder.get_entity_associations(entities[0][0])\n",
    "assert associations == {\"topic's main Wikimedia portal\": 'Portal:Microsoft',\n",
    " 'founded by': 'Bill Gates',\n",
    " 'country': 'United States of America',\n",
    " 'instance of': 'software company',\n",
    " 'headquarters location': 'Redmond',\n",
    " 'stock exchange': 'NASDAQ',\n",
    " 'chief executive officer': 'Steve Ballmer',\n",
    " \"topic's main category\": 'Category:Microsoft',\n",
    " 'subsidiary': 'Xbox Game Studios',\n",
    " 'described by source': 'Lentapedia (full versions)',\n",
    " 'industry': 'technology industry',\n",
    " 'product or material produced': 'Microsoft Windows',\n",
    " 'legal form': 'Washington corporation',\n",
    " 'business division': 'Microsoft Research',\n",
    " 'history of topic': 'history of Microsoft',\n",
    " 'member of': 'Alliance for Open Media',\n",
    " 'permanent duplicated item': None,\n",
    " 'part of': 'NASDAQ-100',\n",
    " 'award received': 'Big Brother Awards',\n",
    " 'owner of': 'Microsoft TechNet',\n",
    " 'owned by': 'BlackRock',\n",
    " 'board member': 'Reid Hoffman',\n",
    " 'chairperson': 'John W. Thompson',\n",
    " 'location of formation': 'Albuquerque',\n",
    " 'director / manager': 'Satya Nadella',\n",
    " 'external auditor': 'Deloitte & Touche LLP',\n",
    " 'partnership with': 'ID2020'}, 'Error in `dataset_builder.get_entity_associations()`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description\n",
    "text = \"Darth Vader cut off Luke Skywalker's hand\"\n",
    "ds_builder.description(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
