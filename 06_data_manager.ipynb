{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from kirby.run_params import RunParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manager\n",
    "\n",
    "> Prepares and Loads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataManager():\n",
    "    def __init__(self, run_params):\n",
    "        self.run_params = run_params\n",
    "        self.block_size = 128\n",
    "    # Load, Tokenize, and Augment data\n",
    "    def prepare_data(self):\n",
    "        train_ds, val_ds = map(self.prepare_ds, ('train', 'valid'))\n",
    "        return train_ds, val_ds\n",
    "    \n",
    "    def prepare_ds(self, split):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(self.run_params.model)\n",
    "        tokenizer.pad_token = tokenizer.eos_token \n",
    "        split = f'{split}[:{self.run_params.batch_size if self.run_params.debug else f\"{self.run_params.data_set_percentage}%\"}]'\n",
    "        ds = load_dataset('text', data_files=self.run_params.data_files, split=split)\n",
    "        ds = ds.map(self.tokenize, batched=True, num_proc=4, remove_columns=['text'], fn_kwargs={'tokenizer':tokenizer})\n",
    "        ds = ds.map(\n",
    "            self.group_texts,\n",
    "            batched=True,\n",
    "#             batch_size=self.block_size,\n",
    "#             num_proc=self.run_params.num_workers\n",
    "        )\n",
    "        ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        return ds\n",
    "\n",
    "    # Tokenize a sequence\n",
    "    def tokenize(self, x, tokenizer=None):\n",
    "        tokens = tokenizer(x['text'])\n",
    "        return tokens\n",
    "    \n",
    "    def group_texts(self, examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "            # customize this part to your needs.\n",
    "        total_length = (total_length // self.block_size) * self.block_size\n",
    "        if total_length == 0:\n",
    "            total_length = self.block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + self.block_size] for i in range(0, total_length, self.block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-04bff418a63932f2\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-04bff418a63932f2/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf23b059ec04393b60fa718ea318dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-04bff418a63932f2\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-04bff418a63932f2/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcfca3572df44d4bccd3763fe1dbb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creation\n",
    "from datasets import Dataset\n",
    "run_params = RunParams()\n",
    "data_manager = DataManager(run_params)\n",
    "train_ds, valid_ds = data_manager.prepare_data()\n",
    "assert isinstance(train_ds, Dataset)\n",
    "assert isinstance(valid_ds, Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'input_ids': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,  2311,    73,\n",
       "         13090,   645,   569, 18354,  7496,   513,  1058,   791, 47398, 17740,\n",
       "           357,  4960,  1058, 10545,   230,    99,   161,   254,   112,  5641,\n",
       "         44444,  9202, 25084, 24440, 12675, 11839,    18,   837,  6578,   764,\n",
       "           569, 18354,  7496,   286,   262, 30193,   513,  1267,   837,  8811,\n",
       "          6412,   284,   355,   569, 18354,  7496, 17740,  6711,  2354,  2869,\n",
       "           837,   318,   257, 16106,  2597,  2488,    12,    31,  2712,  2008,\n",
       "           983,  4166,   416, 29490,   290,  6343,    13, 44206,   329,   262,\n",
       "         14047, 44685,   764, 28728,   287,  3269,  2813,   287,  2869,   837,\n",
       "           340,   318,   262,  2368,   983,   287,   262,   569, 18354,  7496,\n",
       "          2168,   764, 12645,   278,   262,   976, 21748,   286, 16106,   290,\n",
       "          1103,  2488,    12,    31,   640, 11327,   355,   663, 27677,   837,\n",
       "           262,  1621,  4539, 10730,   284,   262,   717,   983]),\n",
       " 'labels': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,  2311,    73,\n",
       "         13090,   645,   569, 18354,  7496,   513,  1058,   791, 47398, 17740,\n",
       "           357,  4960,  1058, 10545,   230,    99,   161,   254,   112,  5641,\n",
       "         44444,  9202, 25084, 24440, 12675, 11839,    18,   837,  6578,   764,\n",
       "           569, 18354,  7496,   286,   262, 30193,   513,  1267,   837,  8811,\n",
       "          6412,   284,   355,   569, 18354,  7496, 17740,  6711,  2354,  2869,\n",
       "           837,   318,   257, 16106,  2597,  2488,    12,    31,  2712,  2008,\n",
       "           983,  4166,   416, 29490,   290,  6343,    13, 44206,   329,   262,\n",
       "         14047, 44685,   764, 28728,   287,  3269,  2813,   287,  2869,   837,\n",
       "           340,   318,   262,  2368,   983,   287,   262,   569, 18354,  7496,\n",
       "          2168,   764, 12645,   278,   262,   976, 21748,   286, 16106,   290,\n",
       "          1103,  2488,    12,    31,   640, 11327,   355,   663, 27677,   837,\n",
       "           262,  1621,  4539, 10730,   284,   262,   717,   983])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
